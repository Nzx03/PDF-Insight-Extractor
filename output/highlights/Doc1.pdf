role of Data Analytics?
Sources of Data for Data Analysis
Data Analytics has a key role in improving your business as it is used to gather
hidden insights, generate reports, perform market analysis, and improve
business requirements.
• Gather Hidden Insights – then analyzed with respect
• Generate Reports on to the respective
• Perform Market Analysis understand the strengths and
• Improve Business Requirement improving Business to customer requirements
1. Primary data
2. Secondary data
data which is Raw, original, and extracted directly from the official sources is
The data which is Raw, original, and extracted directly from the official sources is
known as primary data.
Few methods of collecting primary data:
Secondary data is the data which has already been collected and reused again for
some valid purpose.
has two types of sources named internal source and external source.
Internal source:
External source: The data which can’t
The data which can’t be found at internal organizations and can be gained through
external third party resources is external source data.
Other sources:
1.Primary data:
1. Interview method:
2. Survey method:
3. Observation method:
4. Experimental method:
2. Secondary data:
it
it
These types of data can easily be found within the organization such as market
record, a sales record, transactions,
Examples of
external sources are Government publications, news publications,
• Sensors data:
• Satellites data:
• Web traffic:
sources of data can be classified into two types: statistical and non-statistical. Statistical sources refer to data that is gathered for some official purposes,
Non-statistical sources purposes or for the private
refer to the collection of data for other administrative purposes or sector.
different sources of data?
1. Internal sources
2. External sources
data is collected from reports and records of the organisation itself, are known as the internal sources.
data is collected from sources outside the organisation, external sources. For example, if a tour and travel company
Types of Data
A) Primary data
B) Secondary data
first-hand information collected by an investigator.
refers to second-hand information.
the population census conducted by the government of India ten years is primary data.
after every ten years is primary data.
For example, the after every ten years
For example, the address of a person taken from the telephone directory or the phone number of a company taken from Just Dial are secondary data.
the phone number of a company taken from Just Dial are secondary data.
Methods of Collecting Primary Data
Statistical sources refer to data that is gathered for some official purposes, incorporate censuses, and officially administered surveys. Non-statistical sources
1. Direct personal investigation 2. Indirect oral investigation
2. Indirect oral investigation 3. Information through correspondents
3. Information through correspondents 4. Telephonic interview
4. Telephonic interview
Structured, Semi-structured, and
Unstructured Data
Structured Data?
This type of data consists of various addressable elements to encourage effective analysis. The structured form of data gets organized into a repository (formatted)
analysis. that acts
Relational data is one of the most Data.
commendable examples of Structured Data.
Semi-Structured Data?
It is the type of information and data that does not get stored in a relational type of database but has organizational properties that facilitate an easier analysis. In other
database but has organizational properties that facilitate an easier analysis. words, it is not as organized as the structured data but still has a better organization
data. But overall, they ease the space available for the XML data is an example of semi-structured data.
contained information. XML data is an example of semi-structured data.
Unstructured Data?
Various organizations use unstructured data for various apps and analytics. A few examples of the unstructured data
business intelligence apps and analytics. A few examples of the unstructured data structure are Text, PDF, Media logs, Word, etc.
structure are Text, PDF, Media logs, Word,
Structured, Semi-structured, and Untructured Data
5. Mailed questionnaire 6. The questionnaire filled
6. The questionnaire filled by enumerators
type of data structure that does not exist in a predefined organized manner. In words, it does not consist of any predefined data model. As a result, the
other words, it does not consist of any predefined data model. unstructured data is not at all fit for the relational database used
Structured Data
Semi-Structured Data
Data
Unstructured Data
The information and data have a
and data have a predefined
predefined organization.
organization.
The contained data and information have
and information have organizational
organizational properties- but
properties- but are different from
different from predefined structured
predefined structured data.
data.
There is no predefined
predefined organization
organization for the available data and
available data and information in the
information in the system or
system or database.
database.
Data Structure
Structure
Technology Used
Used
Flexibility
less flexibility.
flexibility.
more flexible the structured
than the structured data.
data.
most flexible of
all.
adapts the transaction from
transaction from DBMS. It is not of
DBMS. mature
not of type.
mature type.
consists of no management of
management of transaction or
transaction or concurrency.
concurrency.
possible to version over
version over tables, rows,
tables, rows, and tuples.
tuples.
possible to version over graphs
version over graphs or tuples.
or tuples.
possible to version the data
version the data as a whole.
a whole.
very robust in nature.
nature.
is a fairly new technology. Thus,
technology. Thus, it is not very robust in
not very robust in nature.
nature.
Robustness Structured very
Management
of Version
Management of Version
Management of Version
Management of
of Transaction
Transaction
Structured Data words on the basis
words on the basis of relational
of relational database tables.
database tables.
Semi-Structured Data works on the basis of
works on the basis of Relational Data
Relational Data Framework (RDF)
Framework (RDF) or XML.
XML.
Unstructured data works on the basis
works on the basis of binary data and
of binary data and the available
the available characters.
characters.
It has a mature type of
type of transaction.
transaction. there are various
Scaling a database schema is very
schema is very difficult. Thus, a
difficult. structured
Scaling a Semi- Structured type
Structured type of data is comparatively
data is comparatively much more feasible.
much more feasible.
An unstructured data type is the
data type is the most scalable in
most scalable in nature.
nature.
Performance of Query
of Query
Scalability
Semi-structured queries over various
queries over various nodes (anonymous)
nodes (anonymous) are most definitely
are most definitely possible.
possible.
Unstructured data only allows textual
only allows textual types of queries.
types of queries.
# Five Characteristics Of Good Quality Data!
1. Data should be precise which means it should contain accurate information. Precision saves time of the user as well as their
information. money.
2. Data should be relevant and according to the requirements of the user. Hence the legitimacy of the data should be checked before
user. considering
3. Data should be consistent and reliable. incomplete data or no data at all.
4. Relevance of data is necessary in order for it to be of good quality and useful. Although in today’s world of dynamic data
quality and useful. relevant information
5. A high quality data is unique to the requirement of the user. Moreover it is easily accessible and could be processed further
Data?
The quantities, characters, or symbols on which operations are performed by a computer, which may be stored and transmitted in the form of electrical signals and recorded on
which may be stored and transmitted in the form of electrical signals and recorded on magnetic, optical, or mechanical recording media.
magnetic, optical, or mechanical recording media.
Big Data?
Big Data is a collection of data that is huge in volume, yet growing exponentially with time. It is a data with so large size and complexity that none of traditional data
time. management
It is a data with so large size and complexity that none of traditional data management tools can store it or process it efficiently. Big data is also a data but
management tools can store it or process it efficiently. Big data is also a data but with huge size.
huge size.
Example of Big Data?
The New York Stock Exchange is an example of Big Data that generates about one terabyte of new trade data per day.
terabyte of new trade data per day.
The statistic shows that 500+terabytes of new data get ingested into the databases of social media site Facebook, every day. This data is mainly generated in terms of photo
social media site Facebook, every day. and video uploads, message exchanges,
A single Jet engine can generate 10+terabytes of data in 30 minutes of flight time. With many thousand flights per day, generation of data reaches up to many Petabytes.
many thousand flights per day, generation of data reaches up to many Petabytes.
Types Of Big Data
Characteristics Of Big Data
1. Structured 2. Unstructured
2. Unstructured 3. Semi-structured
3. Semi-structured
• Volume Variety
• Variety Velocity
• Velocity Variability
• Variability
Big Data itself is related to a size which is enormous. role in determining value out of data. Also, whether a particular
‘Volume’ is one characteristic which needs to be considered while dealing with Big solutions.
Data solutions.
(ii) Variety – The next aspect of Big Data is its variety.
Variety refers to heterogeneous sources and the nature of data, both structured and unstructured. During earlier days, spreadsheets and databases were the only sources
unstructured. data considered
(iii) Velocity data is generated
‘velocity’ refers processed to meet
(iii) Velocity data is
(ii)
refers to the speed of generation of data. How meet the demands, determines real potential
Big Data Velocity deals with the speed at which data flows in from sources like business processes, application logs, networks, and social media sites, sensors, Mobile devices, etc.
processes, application logs, networks, and social media sites, sensors, Mobile devices, etc. The flow of data is massive and continuous.
(iv) Variability thus hampering
This refers to the inconsistency which can be shown by the data at times, the process of being able to handle and manage the data effectively.
thus hampering the process of being able to handle and manage the data effectively.
Advantages Of Big Data Processing
• Businesses can utilize outside intelligence while taking decisions
• Improved customer service
• Early identification of risk to the product/services, if any Better operational efficiency
• Better operational efficiency
(i) Volume plays a very
Need / Importance of Big Data
• Reduction in cost.
• Time reductions.
• New product development with optimized offers.
• Well-groomed decision making.
Challenges of Big data
• Rapid Data Growth:
• Storage:
• Unreliable Data:
• Data Security:
Introduction to Big Data platform
platform
Introduction to Big Data platform
1. Big data platform is a type of IT solution that combines the features and capabilities of
several big data application and utilities within a single solution.
an enterprise class IT platform that enables organization in developing,
deploying,
operating
managing a big data infrastructure/ environment.
1.
2. It is an
Features of Big Data analytics platform
# Steps involved in data analysis are
Data Requirements Specification
the data necessary as inputs to the
analysis is identified (e.g., Population of people).
Specific variables regarding a
population (e.g., Age and Income) may be specified and obtained. Data may be
numerical or categorical.
Data Collection is the process of gathering information on targeted variables identified
as data requirements.
1. Big Data platform should be able to accommodate new platforms and tool based on the
business requirement.
2. It should support linear scale-out.
3. It should have capability for rapid deployment.
4. It should support variety of data format.
5. Platform should provide data analysis and reporting tools.
6. It should provide real-time data analysis software.
7. It should have tools for searching the data through large data sets.
• Data Requirements Specification
• Data Collection
• Data Processing
• Data Cleaning
• Data Analysis
• Communication
Data Collection
Data is collected from various sources ranging from organizational databases to the
information in web pages.
Data Processing
The data that is collected must be processed or organized for analysis.
For example, the data
might have to be placed into rows and columns in a table within a Spreadsheet or
Statistical Application.
Data Cleaning
The processed and organized data may be incomplete, contain duplicates, or contain
errors. Data
Data Cleaning is the process of preventing and correcting these errors.
For example,
while cleaning the financial data, certain totals might be compared against reliable
published numbers or defined thresholds.
Data Analysis
Data that is processed, organized and cleaned would be ready for the analysis.
Statistical Data Models such as Correlation, Regression Analysis can be used to
identify the relations among the data variables.
The process might require additional Data Cleaning or additional Data Collection, and
hence these activities are iterative in nature.
Communication
The results of the data analysis are to be reported in a format as required by the users
to support their decisions and further action.
Evolution of analytics scalability
we have to pull the data together in a separate
analytics environment and then start performing analysis.
1.
2. Analysts do the merge operation on the data sets which contain rows and
columns.
3. The columns represent information about the customers such as name,
spending level, or status.
4. In merge or join, two or more data sets are combined together.
5. Analysts also do data preparation.
Data preparation is made up of joins,
aggregations, derivations, and transformations.
6. Massively Parallel Processing (MPP) system is the most mature, proven, and
widely deployed mechanism for storing and analyzing large amounts of data.
7. An MPP database breaks the data into independent pieces managed by
independent storage and central processing unit (CPU) resources.
Fig. Massively parallel processing system data storage
#Data analytic tools.
#1 Tableau Public
Tableau,
tools, is a simple and intuitive and tool
which offers intriguing insights through data visualization.
Uses of Tableau Public
You can publish interactive data visualizations to the web for free.
No programming skills required.
2.
1.
8. MPP systems build in redundancy to make recovery easy.
9. MPP systems have resource management tools :
a. Manage the CPU and disk space
b. Query optimizer
3. Visualizations published to Tableau Public can be embedded into blogs and
web pages and be shared through email or social media. The shared
content can be made available s for downloads.
Limitations of Tableau Public
All data is public and offers very little scope for restricted access
2. Data size limitation
1.
3. Cannot be connected to
4. The only way to read is via OData sources, is Excel or txt.
#2 OpenRefine
Formerly known as GoogleRefine,
the data cleaning software that helps you clean
up data for analysis. It
Uses of OpenRefine
1.
Cleaning messy data
2. Transformation of data
3. Parsing data from websites
4. Adding data to the dataset by fetching it from web services. For instance,
OpenRefine could be used for geocoding addresses to geographic
coordinates.
Limitations of OpenRefine
Open Refine is unsuitable for large datasets.
2. Refine does not work very well with big data.
1.
#3 KNIME
KNIME,
helps you to manipulate,
helps
analyze, and model data through visual programming.
Uses of KNIME
Rather than writing blocks of code, you just have to drop and drag
connection points between activities.
1.
2. This data analysis tool supports programming languages.
3. In fact, analysis tools like these can be extended to run chemistry data, text
mining, python, and R.
Limitation of KNIME
Poor data visualization
#4 RapidMiner
RapidMiner provides machine learning procedures and data mining including
data visualization, processing, statistical modeling, deployment, evaluation, and
predictive analytics.
Uses of RapidMiner
It provides an integrated environment for business analytics, predictive analysis,
text mining, data mining, and machine learning.
Limitations of RapidMiner
RapidMiner
1.
RapidMiner has size constraints with respect to the number of rows.
2. For RapidMiner, you need more hardware resources than ODM and SAS.
#5 Google Fusion Tables
An incredible tool for data
analysis, mapping, and large dataset visualization,
Uses of Google Fusion Tables
1.
Visualize bigger table data online:
2. Filter and summarize across hundreds of thousands of rows.
3. Combine tables with other data on the web:
Limitations of Google Fusion Tables
Only the first 100,000 rows of data in a table are included in query results
or mapped.
2. The total size of the data sent in one API call cannot be more than 1MB.
1.
#6 NodeXL
NodeXL is a free and open-source network analysis and visualization software.
Uses of NodeXL
1. Data Import
2. Graph Visualization
3. Graph Analysis
4. Data Representation
It can import various graph formats like adjacency
matrices, Pajek .net, UCINet .dl, GraphML, and edge lists.
lists.
Limitations of NodeXL
1.
Multiple seeding terms are required for a particular problem.
2. Need to run the data extractions at slightly different times.
#7 Wolfram Alpha
Wolfram Alpha,
knowledge engine or answering engine founded by Stephen Wolfram.
you get answers to factual queries directly by computing the
answer from externally sourced ‘curated data’ instead of providing a list of
documents or web pages.
Uses of Wolfram Alpha
1.
Is an add-on for Apple’s Siri
2. Provides detailed responses to technical searches and solves calculus
problems.
3. Helps business users with information charts and graphs, and helps in
creating topic overviews, commodity information, and high-level pricing
history.
Limitations of Wolfram Alpha
1.
Wolfram Alpha can only deal with the publicly known number and facts, not
with viewpoints.
2. It limits the computation time for each query.
#8 Google Search Operators
It is a powerful resource that helps you filter Google results instantly to get the
most relevant and useful information.
Uses of Google Search Operators
1.
Fast filtering of Google results.
2. Google’s powerful data analysis tool can help discover new information or
market research.
#9 Solver
Excel Solver?
tools is a linear programming and optimization tool in excel.
It is an advanced optimization tool that helps in quick problem-
solving.
Uses of Solver
The final values found by Solver are a solution to interrelation and decision.
Limitations of Solver
1.
Poor scaling is one of the areas where Excel Solver lacks.
2. It can affect solution time and quality.
3. Solver affects the intrinsic solvability of your model.
#10 Dataiku DSS
Dataiku is a collaborative data
science software platform that helps the team build, prototype, explore, and
deliver their own data products more efficiently.
Uses of Dataiku DSS
It provides an interactive visual interface where they can build, click, and point or
use languages like SQL. This
It
data analytics tool lets you draft data preparation and
modulization in seconds. Helps you coordinate development and operations by
handling workflow automation, creating predictive web services, model health on
a daily basis, and monitoring data.
Limitation of Dataiku DSS
1.
Limited visualization capabilities
2. UI hurdles: Reloading of code/datasets
3. Inability to easily compile entire code into a single document/notebook
4. Still, need to integrate with SPARK
Process Analysis
Process Analysis is nothing but a review of the entire process flow of an organization
to arrive at a thorough understanding of the process.
Objectives of Process analysis
1. Identify the factors that make it difficult to understand the process.
3. Remove bottlenecks
5. Ascertain the allocation of resources
# Analytics and Reporting
• Analytics involves data interpreting where reporting involving presenting factual, accurate data.
factual, accurate data.
#Data Analytics Applications Some of the different data analytics
1. Security Data analytics
2. Transportation Data analytics can be
3. Risk detection
4. Risk Management Risk management is an
Management management is an
5. Delivery Several top logistic
6. Fast internet allocation While it might seem that allocating
7. Reasonable Expenditure When one is building Smart cities,
8. Interaction with customers In insurance, there should be a
9. Planning of cities One of the untapped disciplines
10. Healthcare While medicine
11. For Travelling If you ever thought
12. Managing Energy Many firms engaging with
13. Internet searching When you use Google, you
14. Digital advertisement Data analytics has revolutionized
Key roles for a successful analytics project
1. Business User :
2. Project Sponsor
3. Project Manager
4. Business Intelligence Analyst
5. Database Administrator (DBA)
6. Data Engineer
7. Data Scientist
# Stages of data analytics /Life cycle of data analytics
Data Analytics Lifecycle
Phase 1: Discovery
Phase 2: Data Preparation
Phase 3: Model Planning
Phase 4: Model Building
Phase 5: Communication Results
Phase 6: Operationalize